\subsection{Методы оптимизации гиперпараметров}
Несмотря на то, что эта тема, в сущности, не относится к методам глубокого обучения, она была включена в рассмотрение из-за своей широкой сферы применения и возможности значительного улучшения результатов моделей глубокого обучения.

Гиперпараметр -- это характеристика модели, задаваемая извне и которая не может быть получена на основе данных. Так, в сверточных сетях, например, гиперпараметрами являются размер фильтров и шаг их перемещения. Значение гиперпараметров должно быть задано перед началом процесса обучения.

Задача оптимизации гиперпараметров\cite{hyperparamopt-overview} заключается в выборе набора оптимальных значений гиперпараметров для заданного алгоритма машинного обучения. Существует несколько подходов к решению этой задачи, в числе которых: сеточный поиск, случайный поиск, оптимизация на основе градиентов и другие.
Мы будем рассматривать сеточный и случайный поиск. Сеточный поиск является традиционным способом подбора оптимального решения. Его суть заключается в простом переборе всевозможных значений на заданном пространстве допустимых гиперпараметров. Обычно оценка эффективности сеточного поиска производится путем перекрестной проверки на наборе тренировочных данных, либо выполнением модели на специально выделенном валидационном наборе данных.

Поскольку пространство гиперпараметров может включать в себя действительные и неограниченные числа, перед началом подбора необходимо задать ограничения и шаг сетки.

Случайный поиск концептуально очень похож на сеточный. Основным отличием является то, что при случайном поиске перебор гиперпараметров ведется не последовательно, а на каждом шаге выбираются случайные значения\cite{hyperparamopt-random}. В случае, когда лишь небольшое число гиперпараметров оказывает значительное влияние на работу алгоритма машинного обучения, этот метод находит решение гораздо быстрее сеточного. Еще одним преимуществом случайного поиска является легкость применения параллельных вычислений.
